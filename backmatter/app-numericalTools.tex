% !TeX root = ../msc_thesis_jayd.tex
\chapter{Numerical Tools}

\section{Numerical Computation of the Scattering Matrix in \gls{sqa}}\label{sec:app.numTools.scatMat}
We detail the numerical solution of Helmholtz's equation with the onion discretization 
procedure of \gls{sqa}.

In the TE case, we must take $n\mapsto n_\text{eff}$. This leads us to evaluate the 
extra integral
  \begin{equation}
   I = \frac{1}{2\pi}\int_0^{2\pi}\left[\frac{1}{n}\frac{d^2n(r_j,\theta)}{d\theta^2}-\frac{2}{n^2}\left(\frac{dn(r_j,\theta)}{d\theta}\right)^2\right]e^{i(m-m')\theta}d\theta.
  \end{equation}
The numerical cost of this integral can be lessened 
if we notice that if we write it as a function of $n^2$ we obtain
  \begin{equation}
   I = \frac{1}{2\pi}\int_0^{2\pi}\left[\frac{1}{2n^2}\frac{d^2n^2(r_j,\theta)}{d\theta^2}-\frac{3}{4n^4}\left(\frac{dn^2(r_j,\theta)}{d\theta}\right)^2\right]e^{i(m-m')\theta}d\theta
  \end{equation}
which, in turn, can be rewritten as
  \begin{equation}
   I = \frac{1}{2\pi}\int_0^{2\pi}\left[\frac{1}{2}\frac{d^2\log n^2(r_j,\theta)}{d\theta^2}-\left(\frac{1}{2}\frac{d\log n^2(r_j,\theta)}{d\theta}\right)^2\right]e^{i(m-m')\theta}d\theta.
  \end{equation}
Integrating by parts twice yields
  \begin{equation}
   I = \frac{1}{2\pi}\int_0^{2\pi}\left[-\frac{(m-m')^2}{2}\log n^2(r_j,\theta)-\left(\frac{1}{2}\frac{d\log n^2(r_j,\theta)}{d\theta}\right)^2\right]e^{i(m-m')\theta}d\theta.
  \end{equation}
The second term cannot be integrated out, but this final result means that
we will only need to numerically evaluate the first derivative of the 
refractive index (although its analytical form could be provided).

Notice the form of the $\mat{L}^j$ matrix. As can be seem from inspection, 
the value of the elements depend only on their distance from the diagonal.
Taking a closer look reveals the form
  \begin{equation}
   \mat{L}^j = \mat{M}^2 +k^2r_j^2\begin{pmatrix} 
		  n_0 & n_{-1} & \cdots & \cdots & n_{-2M}	\\
		  n_1	  & n_0& \cdots & \cdots & n_{-2M-1}\\
		  n_2	  & n_1	   & n_0& \cdots & n_{-2M-2}\\
		  \vdots  & n_2    & n_1    & \ddots & \vdots   \\
		  n_{2M}  & \cdots & \cdots & \cdots & n_0
		\end{pmatrix}
  \end{equation}
which is manifestly Toeplitz. When the potential is real, the Fourier
series has the property $n_{-j}=n_j^*$, which makes the $\mat{L}^j$ 
matrix Hermitian. In the general case, however, it is not. Because
we will need to use orthogonality relations in what follows, we must
compute both the left and right eigenvectors\index{left eigenvectors}. 
We will note the left (covariant) basis by $\Ket{\tilde{\Phi}_\mu^j}$.
This is not sufficient, however, because we are not guaranteed that
both sets of eigenvectors will form a complete basis. 

\paragraph{Normality of $\mat{L}^j$}

\section{Vector Scattering and the Variable Phase Method}\label{sec:app.numTools.vpm}
The \gls{vpm} is based upon an eigenfunction expansion of the 
angular part of the scattering equations. This reduces \gls{pde}
to a matrix \gls{ode} for the scattering amplitudes. In what
follows, we will derive those matrix \glspl{ode} for both 
2D scalar and 3D vector scattering. 

\subsection{2D Scalar Scattering}
Before applying the expansions, we will multiply the equation through 
by $n$ and write the derivatives explicitly
  \begin{equation}
    \left[n\left(\frac{\partial^2}{\partial r^2}+\frac{1}{r}\frac{\partial}{\partial r}+\frac{1}{r^2}\frac{\partial^2}{\partial\theta^2}\right)+k^2n^3\right]H_z
    =
    2\frac{\partial H_z}{\partial r}\frac{\partial n}{\partial r}+\frac{2}{r^2}\frac{\partial H_z}{\partial\theta}\frac{\partial n}{\partial\theta}.
  \end{equation}
Expanding this equation yields (after some simplification)
  \begin{multline}
   \sum_{m,m'}
    \left\{\frac{n_{m'}}{2\pi}\left(\frac{\psi_{m}''}{r^{1/2}}-\frac{(m^2-1/4)}{r^{5/2}}\psi_m\right)e^{i(m+m')\theta}\right\}
	+\left(\frac{k}{2\pi}\right)^2\left(\sum_{m'}n_{m'}e^{im'\theta}\right)^3\left(\sum_m\frac{\psi_m}{r^{1/2}}e^{im\theta}\right)\\
	=\frac{2}{2\pi}\sum_{m,m'}\left\{n_{m'}'\left[\frac{\psi_m'}{r^{1/2}}-\frac{\psi_m}{2r^{3/2}}\right]-\frac{mm'}{r^2}\frac{\psi_mn_{m'}}{r^{1/2}}\right\}e^{i(m+m')\theta}
  \end{multline}
Projecting onto $\sqrt{r}e^{-im''\theta}/\sqrt{2\pi}$
  \begin{multline}
    \sum_{m,m'}\int_0^{2\pi}\left\{\frac{n_{m'}}{(2\pi)^{3/2}}\left(\psi_m''-\frac{(m^2-1/4)\psi_m}{r^2}\right)+\frac{k^2c_{m'}\psi_m}{(2\pi)^{3/2}}\right\}e^{i(m+m'-m'')\theta}d\theta\\
    \frac{2}{(2\pi)^{3/2}}\int_0^{2\pi}\left\{n_{m'}'\left[\psi_m'-\frac{\psi_m}{2r}\right]-\frac{mm'}{r^2}\psi_mn_{m'}\right\}e^{i(m+m'-m'')\theta}d\theta
  \end{multline}
where
  \begin{equation}
   c_{m'} = \sum_{m}\sum_{m''}n_mn_{m''}n_{m'-m''-m}.
  \end{equation}
Integrating, introducing the matrices
  \begin{subequations}
  \begin{align}
   \left[\mat{N}(r)\right]_{mm''}	&= \sum_{m'}n_{m'}\delta_{m+m',m''}	\\
   \left[\mat{Z}(r)\right]_{mm''}	&= \sum_{m'}mm'n_{m'}'\delta_{m+m',m''},
  \end{align}
  \end{subequations}
we obtain the matrix ODE
  \begin{equation}
  \label{eq:vpm.TE.GeneralCase}
  \bo{\psi}''-\frac{\mat{M^2}-\mat{I}/4}{r^2}\bo{\psi}+\frac{k^2}{2\pi}\mat{N}^2\bo{\psi}
    =
   2\mat{N}^{-1}\mat{N}'\left[\bo{\psi}'-\frac{\psi}{2r}\right]-\frac{2}{r^2}\mat{N}^{-1}\mat{Z}\bo{\psi}
  \end{equation}
In the free case, i.e. $n(r,\theta)=1$, we have that
  \begin{equation}
   n_{m'} = \sqrt{2\pi}\delta_{m'0}
  \end{equation}
such that we have
  \begin{equation}
    \label{eq:vpm.freeCase}
    \bo{\psi}''-\frac{\mat{M}^2-\mat{I}/4}{r^2}\bo{\psi}+k^2\bo{\psi}=0,
  \end{equation}
the Bessel equation. For the general case, it makes sense
to posit a matrix solution of the form
  \begin{equation}
   \bo{\psi}=\mat{G}_k(r)\mat{W}(kr)
  \end{equation}
where $\left[\mat{W}(kr)\right]_{mm}=H_m^{(\pm)}(kr)$ where
we choose the sign for either incoming or outgoing 
wave boundary condition. Substituting this into \eqref{eq:vpm.TE.GeneralCase}
and using \eqref{eq:vpm.freeCase} gives
  \begin{multline}
    \label{eq:vpm.masterEquation}
    \mat{G}''+2\mat{G}'\frac{d}{dr}\left(\log\mat{W}\right)+\frac{1}{r^2}\left[\mat{G},\mat{M^2}-\mat{I}/4\right]+\frac{k^2}{2\pi}\left(\mat{N}^2-2\pi\right)
     \\ =
    2\mat{N}^{-1}\mat{N}'\left[\mat{G}'+\mat{G}\frac{d}{dr}\left(\log\mat{W}\right)-\frac{\mat{G}}{2r}\right]-\frac{2}{r^2}\mat{N}^{-1}\mat{ZG}.
  \end{multline}
In the case of TM polarization, the right-hand side 
of \eqref{eq:vpm.masterEquation} vanishes. 

We start with the matrix case. We parametrize the solution as
  \begin{equation}
    \mat{F}_k(r) = \mat{G}_k(r)\mat{W}(kr)
  \end{equation}
where $\mat{W}(kr)$ has the outgoing wave functions $rh_\ell^{(+)}(kr)$ on its diagonal. 
With the Sommerfeld condition, we can conclude that $\mat{G}_k(\infty) = \mat{I}$ and 
$\mat{G}_k'(\infty)=\mat{0}$. We have the differential equation
  \begin{multline}
   -\mat{G}_k(r)''-2\mat{G}_k(r)'\left(\frac{\partial}{\partial r}\ln\mat{W}(kr)\right)
    +\frac{1}{r^2}\left[\mat{L}^2,\mat{G}_k(r)\right]+\mat{V}(r)\mat{G}_k(r)=0
  \end{multline}
The incoming wave solutions solves the same equation, but with
$\mat{W}(kr)\mapsto \mat{W}(kr)^\dagger$. Denoting this solution 
by $\mat{G}_{-k}(r)$, we find that the wave function is
  \begin{equation}
    \bo{\psi}(r) = \mat{G}_{-k}(r)\mat{W}(kr)^\dagger+\mat{G}_k(r)\mat{W}(kr)\mat{S}_k(k)
  \end{equation}
where $\mat{S}_k(k)$ is the scattering matrix. Because of the
$1/r$ factor in our expansion, $\bo{\psi}(0)$ must vanish 
for the field to be finite at the origin. We can then write the 
scattering matrix as
  \begin{equation}
   \mat{S}_k = -\lim_{r\rightarrow0}\mat{W}^{-1}(kr)\mat{G}^{-1}_k(r)\mat{G}_{-k}(r)\mat{W}(kr)^\dagger
  \end{equation}
These equations come from \cite{FOR2012}, modulo some minor changes.

Not surprisingly, we will have to deal with exactly the same issue as before: 
notice the product $\mat{W}^\dagger(kr)\mat{A}\mat{W}^{-1}(kr)$. For diagonal
matrices $\mat{W}$, we can rewrite this product as
  \begin{equation*}
    \mat{A}\circ\left(\text{diag}(\mat{W}^\dagger)\otimes\text{diag}(\mat{W}^{-1})\right)
  \end{equation*}
which is our beloved Schur product that caused some issues in \gls{sqa}. 

\subsection{3D Vector Scattering}
In the electromagnetic case, we start with the curl-curl equation
  \begin{equation}
    \nabla\times\nabla\times\bo{E}-k^2\epsilon(\bo{r})\bo{E}=0.
  \end{equation}
Expanding the field in a tensor spherical harmonics (to be discussed later) yields
an implicit differential equation. In their paper, F+G argue that if they rather
use the equation
  \begin{equation}
   \label{eq:vpm.modCurlCurl}
   \nabla\times\nabla\times\bo{E}(\bo{r})-\epsilon(\bo{r})\nabla\left\{\nabla\cdot\left[\epsilon(\bo{r})\bo{E}(\bo{r})\right]\right\}
      =k^2\epsilon(\bo{r})\bo{E}(\bo{r})
  \end{equation}
which yields the same eigenstates, the resulting differential equation is explicit. 
Interestingly enough, this is exactly the extra term that is used to remove
spurious solutions that appear in variational treatments. 

Recall that we can write Maxwell's equations as the minimization of the energy-like functional
  \begin{equation}
   F\left[\bo{E}\right]=\mathop{\iiint}_V\left[\left(\nabla\times\bo{E}^*\right)\left(\nabla\times\bo{E}\right)-k^2\bo{E}^*\cdot\epsilon(\bo{r})\bo{E}\right]dV.
  \end{equation}
In the variational treatment, this leads to spurious solutions which can provably be removed \cite{KON1976,KOS1984,KOS1985} that adding the null term
$\left(\nabla\cdot\bo{E}^*\right)\left(\nabla\cdot\epsilon(\bo{r})\bo{E}\right)$ in the integrand. 
Taking the first variation of the new functional leads to \eqref{eq:vpm.modCurlCurl}.
  
We now expand both the permittivity $\epsilon(\bo{r})$ and the field in
spherical harmonics. Because the electric field is a vector field, 
we will need to use \textit{vector} spherical harmonics, given by
  \begin{equation}
    \bo{Y}_{jm}^\ell=\sum_{\sigma=-1}^{+1}\sum_{m'=\ell}^{+\ell}C^{jm}_{\ell m'1\sigma}Y_{\ell}^{m'}(\theta,\varphi)\bou{e}_\sigma
  \end{equation}
where $\ell=j,j\pm1$ for the three vector spherical harmonics, $C^{nm}_{n_1m_1,n_2m_2}$
is a Clebsch-Gordan coefficient and the spherical basis vectors $\bou{e}_\sigma$ 
are
  \begin{align}
    \bou{e}_1	&= -\frac{e^{i\varphi}}{\sqrt{2}}\left(\sin\theta\bou{r}+\cos\theta\bou{\theta}+i\bou{\varphi}\right)	\nonumber\\
    \bou{e}_0	&= \cos\theta\bou{r}-\sin\theta\bou{\theta}								\\
    \bou{e}_{-1}&= -\bou{e}_1^*												\nonumber
  \end{align}
Before going any further, let us state the capabilities of this approach. This kind of method
can theoretically deal with arbitrary potentials (permittivities). However, it is best used
when the scale of variations of the potential is not too big compared to the size
of the scatterer. The size of the transforms and matrix ODEs, or in other words the resolution
must be fixed beforehand. For fast variations of the potential, a smaller resolution must be used
and this leads to overall large matrices. For highly non-uniform structures, finite element methods
might be preferable. Radial inhomogeneities can be dealt with variable-step integrators. 
Magnetic materials can be considered by substituting the curl-curl operator $\nabla\times\nabla\cdots$
by $\nabla\times\frac{1}{\mu(\bo{r})}\nabla\times\cdots$. 

\section{Lippmann-Schwinger Computation of the Scattering Matrix}\label{sec:app.numTools.lippmannSchwinger}
\section{Computation of the Logarithmic Derivative $[H^{(\pm)}_\nu(z)]'/H^{(\pm)}_\nu(z)$}\label{sec:app.numTools.logDeriv}

\index{continued fraction expansions|(textbf}
As we have seen from Appendix \ref{sec:app.basicEquationScattering}, the computation of the logarithmic derivative
of Bessel functions is of the utmost importance in the numerical solution 
of scattering problems. Given that we already use Amos' library \cite{AMO86} to evaluate
the Bessel functions, we might have been tempted to use it 
to directly evaluate the derivative. It turns out that using
expansions that pertain to logarithmic derivatives is somewhat
faster and is more accurate than using Amos' library. 

In this section, we introduce some concepts relating to 
continued fraction expansions (CFEs) and discuss their numerical
evaluation. We then derive the CFEs and other expansions that will
be of use in the computation of the logarithmic derivatives.

\subsection{Notation and Necessary Theorems}
A continued fraction expansion is a representation
of a mathemetical function. It can be linked to 
Laurent series, Padé approximants and much more 
\cite{CUY2008}. It has the standard form 
  \begin{equation}
    f = b_0+\cfrac{a_1}{b_1+\cfrac{a_2}{b_2+\cfrac{a_3}{b_3+\cfrac{a_4}{b_4+\cdots}}}}
  \end{equation}
or, more succinctly, 
  \begin{equation}
   f = b_0 + \bigk_{m=1}^\infty\left(\frac{a_m}{b_m}\right)
  \end{equation}
where 'K' is for the German word \textit{Kettenbruch}, meaning continued fraction.
We define the $n$th approximant as
  \begin{equation}
   f_n = b_0 + \bigk_{m=1}^n\left(\frac{a_m}{b_m}\right).
  \end{equation}
We will be concerned with their numerical evaluation and convergence properties.

\index{continued fraction expansions!numerical evaluation of}
Notice that naïvely evaluating the CFE from right-to-left, as a person would do, 
does not yield a satisfying numerical algorithm, as the amount of iterations 
must be fixed in advance and consequently does not allow the control the 
accuracy of the evaluation. The chosen method is taken from the Holy Bible
of Numerics, \textit{Numerical Recipes} \cite{PRE2007} and is called 
the modified Lentz's method. It constructs a rational approximation
of the $n$th approximant 
  \begin{equation}
    f_n = \frac{A_n}{B_n}
  \end{equation}
where 
  \begin{align}
   A_{-1} 	&=1 			& B_{-1} 	&=0	\nonumber\\
   A_0		&= b_0			& B_0		&=1	\\
   A_j		&=b_jA_{j-1}+a_jA_{j-2}	& B_j 		&=b_jB_{j-1}+a_jB_{j-2}.\nonumber
  \end{align}
This method can lead to over/underflow of the floating-point representation: 
the method hence uses 
  \begin{align}
    C_j &= A_j/A_{j-1}			&	D_j	&= B_{j-1}/B_j	\nonumber\\
    	&= b_j+\frac{a_j}{C_{j-1}}	&		&= \frac{1}{b_j+a_jD_{j-1}}\\
    f_j	&= f_{j-1}C_jD_j.\nonumber
  \end{align}
The method is aptly described by Algorithm \ref{algo:app.numTools.cfeEvaluation}.
It allows for a left-to-right evaluation of the CFE and control of the relative
accuracy of the computation. 

  \begin{algorithm}
   \KwData{\texttt{tiny} = square root of smallest representable number}
   \KwData{\texttt{eps} = accuracy of the CFE}
   \eIf{$b_0 = 0$}{$f_0\leftarrow$\texttt{tiny}}{$f_0\leftarrow0$}
   $C_0 \leftarrow f_0$\;
   $D_0 \leftarrow 0$\;
   \Repeat( from $j=1$){$|\Delta_j-1|<$\texttt{eps}}%
   {
    $D_j \leftarrow b_j+a_jD_{j-1}$\;
    \If{$D_j=0$}{$D_j\leftarrow$\texttt{tiny}}
    $C_j \leftarrow b_j+\frac{a_j}{C_{j-1}}$\;
    \If{$C_j=0$}{$C_j\leftarrow$\texttt{tiny}}
    $D_j \leftarrow 1/D_j$\;
    $\Delta_j\leftarrow C_jD_j$\;
    $f_j \leftarrow f_{j-1}\Delta_j$
   }
   \Return{$f_j$}
  \caption{Evaluation of Continued Fractions}
  \label{algo:app.numTools.cfeEvaluation}
  \end{algorithm}

As for the convergence properties, we will only bother with CFEs 
originating from three-term recurrence relations. Indeed, it turns out
that any three-term recurrence relation can be linked to a CFE. 
Consider  
  \begin{equation}
    \label{eq:app.numTools.threeTermRecurrence}
   y_{n+1} + a_ny_n + b_ny_{n-1} = 0. 
  \end{equation}
It can be rewritten as
  \begin{equation}
    \frac{y_n}{y_{n-1}} = -\frac{b_n}{a_n+y_{n+1}/y_n}.
  \end{equation}
Iterating yields the CFE
  \begin{equation}
   \label{eq:app.numTools.recurrenceCFE}
   \frac{y_n}{y_{n-1}} = \bigk_{m=n}^\infty\left(\frac{-b_m}{a_m}\right).
  \end{equation}
Given our goal of computing $[H^{(\pm)}_\nu(z)]'/H^{(\pm)}_\nu(z)$
and in light of (ref to recurrence relation of Bessel functions), it seems that
we have won. The next theorem, however, will prove us wrong.
   \begin{thm}[Pincherle's Theorem \cite{CUY2008}]
    If there exists a \textit{minimal solution} $u_n$ of the three-term
    recurrence relation \eqref{eq:app.numTools.threeTermRecurrence}, 
    the associated CFE \eqref{eq:app.numTools.recurrenceCFE} converges
    to $u_n/u_{n-1}$. A solution is said minimal if there exists another
    solution $v_n$ such that
      \begin{equation}
       \lim_{n\rightarrow\infty} \frac{u_n}{v_n} = 0.
      \end{equation}
    $v_n$ is said to be the dominant solution. The minimal solution is unique. 
   \end{thm}

Because the minimal solution of \eqref{eq:app.Bessel.recurrenceBessel} if $J_\nu(z)$, 
we cannot use the associated CFE to compute the logarithmic derivatives
of Hankel functions. Instead, we must look into the links between
Hankel functions and confluent hypergeometric functions. 

\subsection{CFE and Other Expansions}
In this brief foray into the vast subject
of hypergeometric functions, we will introduce Kummer's 
function and its link to the evaluation of the logarithmic
derivative.

Kummer's function solves the differential equation \cite[\S13.1.1]{ABR1965}
  \begin{equation}
    z\frac{d^2y}{dz^2}+(b-z)\frac{dy}{dz}-ay=0
  \end{equation}
and is noted $U(a,b,z)$. It can be shown that that $u_k=(a)_kU(a+k,b,z)$
is the minimal solution of the recurrence \cite{TEM1983}
  \begin{equation}
    u_{n+1} = \frac{2a-b+2n+z}{a-b+n+1}u_n - \frac{a+n-1}{a-b+n+1}u_{n-1}
  \end{equation}
where $(a)_k$ is the Pochhammer symbol. We can hence derive
  \begin{equation}
    \frac{U(a,b,z)}{U(a+1,b,z)} = 2a-b+2+z-\bigk_{m=1}^\infty\left(\frac{(a+m)(b-a-m-1)}{b-2a-2m-2-z}\right).
  \end{equation}
Combined with \cite[\S13.4.23]{ABR1965}
  \begin{equation}
    U(a+1,b,z) = \frac{1}{1+a-b}U(a,b,z) + \frac{z}{a(1+a-b)}U'(a,b,z), 
  \end{equation}
we obtain \cite{CUY2008}
  \begin{equation}
    \frac{dU(a,b,z)/dz}{U(a,b,z)} = -\frac{a}{z}+\frac{a(1+a-b)/z}{2a-b+2+z}_{-}\bigk_{m=1}^\infty\left(\frac{(a+m)(b-a-m-1)}{b-2a-2m-2-z}\right).
  \end{equation}
Given the relation between Kummer's functions and Hankel functions $H^\omega_{\nu}(z)$ \cite[\S13.6.22/23]{ABR1965}
  \begin{equation}
    H^\omega_\nu(z) = \frac{2}{\sqrt{\pi}}e^{-\omega\left[\pi\left(\nu+1/2\right)-z\right]}(2z)^\nu U(\nu+1/2,2\nu+1,-2i\omega z)\qquad (\omega=\pm)
  \end{equation}
we can finally find the CFE
  \begin{equation}
    \label{eq:app.numTools.cfeHankel}
    \frac{dH^\omega_\nu(z)/dz}{H^\omega_\nu(z)} = -\frac{1}{2z}+i\omega+\frac{\omega}{z}\bigk_{m=1}^\infty\left(\frac{\nu^2-(2m-1)^2/4}{2(iz-\omega m)}\right).
  \end{equation}
In our numerical implementation (see next section), we have found that when $|z|<10^{-2}$, convergence is slow. This mirrors the results of \cite{THO1986}.
We thus use the small argument expansions for the Bessel functions (\textit{q.v.} \S \ref{sec:app.Bessel.smallArguments}) to obtain
  \begin{subequations}
  \begin{align}
   \lim_{z\rightarrow0}\frac{dH^\omega_\nu(z)/dz}{H^\omega_\nu(z)}	&= -\frac{\nu}{z}	& (\nu\neq0)	\\
									&= \frac{1}{z}\left[\frac{\pi}{2i\omega}+\gamma+\ln\left(\frac{z}{2}\right)\right]^{-1} & (\nu=0).
  \end{align}
  \end{subequations}

\index{continued fraction expansions|)textbf}

\subsection{Numerical Tests}
We have performed a number of tests to ascertain the performance of our algorithm. 
To test the precision of the algorithm, we have evaluated the CFE for $z\in\{0,10\}$
and $\nu\in\{-100,100\}\in\mathbb{N}$ and compared it to the values obtained via
Amos' library for a range of tolerances. It can be seen that the maximum deviation 
decreases until our set tolerance hits $10^{-12}$ and then plateaus. Because 
convergence of \ref{eq:app.numTools.cfeHankel} is mathematically insured, we 
conclude that Amos's library evaluate the logarithmic derivative up to 
a precision of $10^{-12}$. This is probably due to the propagation 
of errors in the floating point operations, as we use relation
\eqref{eq:app.Bessel.recurrenceDiffBessel}
to evaluate the derivative. 

\begin{figure}
 \centering
 \includegraphics[width=0.7\textwidth]{figs/app-numTools/maxDiff.pdf}
 \caption[Maximum deviation between the CFE and Amos' implementation as a function
	  of the CFE tolerance]%
	 {Maximum deviation between the CFE and Amos' implementation of the Bessel functions
	 as a function of the tolerance of the CFE. This study was performed in the parameter
	 space $z\in\left\{0.1,10\right\}, \nu\in\left\{-100,100\right\}$. We interpret 
	 the plateau in maximum deviation as the error committed by Amos' implementation, i.e.
	 Amos' implementation has a precision of $\sim10^{-10}$ on the evaluation of the logarithmic derivative.}
\end{figure}


\begin{figure}
 \centering
 \begin{subfigure}[t]{0.47\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/app-numTools/timesTolerance.pdf}
  \caption{Performance of the CFE implementation as a function of its tolerance.
	   We measured the ratio of the time it takes to compute the logarithmic derivative
	   at $z=0$ for all orders $\nu\in\{-100,100\}$ with the CFE and Amos' implementation. 
	   When the tolerance of the CFE hits $\sim10^{-5.5}$, the CFE is slower than Amos'
	   implementation.}
 \end{subfigure}
 \begin{subfigure}[t]{0.47\textwidth}
  \includegraphics[width=\textwidth]{figs/app-numTools/timesPerformance.pdf}
  \caption{Performance of the CFE as a function of $z$. When approaching
  $z=0$ (from all sides), it takes a higher number of terms for the 
  CFE to converge, resulting in a slower algorithm. However, at $z=10^{-2}$, we 
  use the small argument form, preserving both precision and performance.}
 \end{subfigure}
 \caption[Performance of the CFE compared to Amos' library's.]
	 {Performance of the CFE compared to that of Amos' library. The CFE is somewhat
	 slower, but can achieve better precision.}
\end{figure}


However, it can be seen that Amos' library is somewhat faster, given its
precision, that our CFE evaluation 
even though it requires three Hankel function evaluations. 

%\section{Clebsch-Gordan Coefficients and Wigner Symbols}
\input{backmatter/appWigner.tex}

\input{backmatter/appSphHarmonics.tex}

 \nocite{*}